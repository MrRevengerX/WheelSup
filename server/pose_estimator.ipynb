{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e9ecc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the nessacary libraries to run the pose estimator\n",
    "\n",
    "#Import mediapipe to be used for the model\n",
    "import mediapipe as mp\n",
    "#Import opencv for rendaring and drawing capabilities\n",
    "import cv2\n",
    "\n",
    "import numpy as np #Handle numpy arrays\n",
    "import pandas as pd #Handle tabular data\n",
    "import os #Handle folder structure\n",
    "import pickle #Save and oad ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4859f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('randomforestclassifier', RandomForestClassifier())])\n"
     ]
    }
   ],
   "source": [
    "#Import the model from the binary file\n",
    "with open('shoulder_press.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    print(\"Model Loaded\")\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b57ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -  PoseLandmark.NOSE\n",
      "1  -  PoseLandmark.LEFT_EYE_INNER\n",
      "2  -  PoseLandmark.LEFT_EYE\n",
      "3  -  PoseLandmark.LEFT_EYE_OUTER\n",
      "4  -  PoseLandmark.RIGHT_EYE_INNER\n",
      "5  -  PoseLandmark.RIGHT_EYE\n",
      "6  -  PoseLandmark.RIGHT_EYE_OUTER\n",
      "7  -  PoseLandmark.LEFT_EAR\n",
      "8  -  PoseLandmark.RIGHT_EAR\n",
      "9  -  PoseLandmark.MOUTH_LEFT\n",
      "10  -  PoseLandmark.MOUTH_RIGHT\n",
      "11  -  PoseLandmark.LEFT_SHOULDER\n",
      "12  -  PoseLandmark.RIGHT_SHOULDER\n",
      "13  -  PoseLandmark.LEFT_ELBOW\n",
      "14  -  PoseLandmark.RIGHT_ELBOW\n",
      "15  -  PoseLandmark.LEFT_WRIST\n",
      "16  -  PoseLandmark.RIGHT_WRIST\n",
      "17  -  PoseLandmark.LEFT_PINKY\n",
      "18  -  PoseLandmark.RIGHT_PINKY\n",
      "19  -  PoseLandmark.LEFT_INDEX\n",
      "20  -  PoseLandmark.RIGHT_INDEX\n",
      "21  -  PoseLandmark.LEFT_THUMB\n",
      "22  -  PoseLandmark.RIGHT_THUMB\n",
      "23  -  PoseLandmark.LEFT_HIP\n",
      "24  -  PoseLandmark.RIGHT_HIP\n",
      "25  -  PoseLandmark.LEFT_KNEE\n",
      "26  -  PoseLandmark.RIGHT_KNEE\n",
      "27  -  PoseLandmark.LEFT_ANKLE\n",
      "28  -  PoseLandmark.RIGHT_ANKLE\n",
      "29  -  PoseLandmark.LEFT_HEEL\n",
      "30  -  PoseLandmark.RIGHT_HEEL\n",
      "31  -  PoseLandmark.LEFT_FOOT_INDEX\n",
      "32  -  PoseLandmark.RIGHT_FOOT_INDEX\n"
     ]
    }
   ],
   "source": [
    "#Display the results of the prediction done by the model\n",
    "draw_helpers = mp.solutions.drawing_utils \n",
    "holistic_model = mp.solutions.holistic\n",
    "\n",
    "#Use hollistics model to list down the landmarks \n",
    "for id, landmark in enumerate(holistic_model.PoseLandmark):\n",
    "    print(id, \" - \", landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b606ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate angle between 3 landmark points\n",
    "def calculate_pose_angle(start_point, mid_point, end_point):\n",
    "    #Convert landmark coords to numpy array\n",
    "#     start_point = np.array(start_point) \n",
    "#     mid_point = np.array(mid_point) \n",
    "#     end_point = np.array(end_point) \n",
    "    \n",
    "    max_angle = 180.0\n",
    "    \n",
    "    #[0] = x, [1] = y, [2] = z\n",
    "    radians = np.arctan2(end_point[1] - mid_point[1], end_point[0] - mid_point[0]) - np.arctan2(start_point[1] - mid_point[1], start_point[0] - mid_point[0])\n",
    "    #Convert to an angle\n",
    "    angle = np.abs(radians * max_angle / np.pi)\n",
    "    \n",
    "    if angle > max_angle:\n",
    "        angle = 360 - angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b47a8aaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m rgb_frame\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m        \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#Use holistic model to make detections\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m result_frame \u001b[38;5;241m=\u001b[39m \u001b[43mholistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#Set frame back to writable format after detection\u001b[39;00m\n\u001b[0;32m     37\u001b[0m rgb_frame\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m   \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mediapipe\\python\\solutions\\holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Display the results of the prediction done by the model\n",
    "draw_helpers = mp.solutions.drawing_utils \n",
    "holistic_model = mp.solutions.holistic\n",
    "\n",
    "#Connect the test video from the device\n",
    "sample_video = cv2.VideoCapture('datasets/IMG_0126.MOV')\n",
    "\n",
    "down = None\n",
    "counter = 0\n",
    "rep_list = []\n",
    "frame_list = []\n",
    "status = None\n",
    "\n",
    "#Load the holistic model\n",
    "with holistic_model.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #Loop through each frame of the video \n",
    "    while sample_video.isOpened():\n",
    "        #Returns the status of the read and the frame as an image\n",
    "        ret, frame = sample_video.read()\n",
    "        \n",
    "        #If frame is read correctly, status is true\n",
    "        if ret == False:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "          \n",
    "        #Recolor the captured frame from BGR to RGB (Medipipe requies frames to be in RGB format)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #Prevent writing and copying frame data to improve performance while making the detection\n",
    "        rgb_frame.flags.writeable = False        \n",
    "        \n",
    "        #Use holistic model to make detections\n",
    "        result_frame = holistic.process(rgb_frame)\n",
    "        \n",
    "        #Set frame back to writable format after detection\n",
    "        rgb_frame.flags.writeable = True   \n",
    "        \n",
    "        #Recolor the captured frame from BGR for rendering with opencv\n",
    "        bgr_frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        #Predict the coordinates of the landmarks (resulrs screen)\n",
    "        try:\n",
    "            pose_landmarks_array = result_frame.pose_landmarks.landmark\n",
    "            \n",
    "            # Filter out only the upper body landmarks\n",
    "            upper_body_landmarks = [pose_landmarks_array[i] for i in [11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]]\n",
    "            \n",
    "            # Format the upper body landmarks into a numpy array for better structuring and collapse the array to 1 dimension\n",
    "            pose_landmarks_nparray = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in upper_body_landmarks]).flatten() \n",
    "                              if result_frame.pose_landmarks else np.zeros(12*4))\n",
    "            \n",
    "            #Pass the numpy array into a data frame\n",
    "            features = pd.DataFrame([pose_landmarks_nparray])\n",
    "            \n",
    "            #Store the top class of the prediction\n",
    "            pose_class_status = model.predict(features.values)[0]\n",
    "            #Store the probability of the prediction\n",
    "            pose_class_status_prob = model.predict_proba(features.values)[0]  \n",
    "            \n",
    "            #Append correct-incorrect probabilitiesto the frame_list\n",
    "            frame_list.append(pose_class_status)\n",
    "            \n",
    "            #Dictionary to store the coords in pixels of the landmarks\n",
    "            points = {}\n",
    "            for id, landmark in enumerate(result_frame.pose_landmarks.landmark):\n",
    "                height, width, center = bgr_frame.shape\n",
    "                cx, cy = int(landmark.x * width), int(landmark.y * height)\n",
    "                points[id] = (cx, cy)\n",
    "\n",
    "            cv2.circle(bgr_frame, points[12], 15, (255,0,0), cv2.FILLED)\n",
    "            cv2.circle(bgr_frame, points[14], 15, (255,0,0), cv2.FILLED)\n",
    "            cv2.circle(bgr_frame, points[16], 15, (255,0,0), cv2.FILLED)\n",
    "            \n",
    "            cv2.circle(bgr_frame, points[11], 15, (255,0,0), cv2.FILLED)\n",
    "            cv2.circle(bgr_frame, points[13], 15, (255,0,0), cv2.FILLED)\n",
    "            cv2.circle(bgr_frame, points[15], 15, (255,0,0), cv2.FILLED)            \n",
    "        \n",
    "            #For Right Arm\n",
    "            if calculate_pose_angle(points[16], points[14], points[12]) >= 158: \n",
    "                down = True\n",
    "            \n",
    "            #For Left Arm\n",
    "            elif calculate_pose_angle(points[15], points[13], points[11]) >= 158:\n",
    "                down = True\n",
    "       \n",
    "            #For Right Arm\n",
    "            if down and calculate_pose_angle(points[16], points[14], points[12]) <= 90: \n",
    "                down = False\n",
    "                counter += 1\n",
    "                rep_list.append(frame_list)\n",
    "                frame_list = []\n",
    "            \n",
    "            #For Left Arm\n",
    "            elif down and calculate_pose_angle(points[15], points[13], points[11]) <= 90:\n",
    "                down = False\n",
    "                counter += 1\n",
    "                rep_list.append(frame_list)\n",
    "                frame_list = []\n",
    "            \n",
    "            #Set a rectangle box to display the results of the prediction in the video frame\n",
    "            #rectangle(container, top_coord, bottom_coord, color, line_thickness)\n",
    "            cv2.rectangle(bgr_frame, (0,0), (600, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            #Display the class label inside the rectangle box\n",
    "            cv2.putText(bgr_frame, 'Class'\n",
    "                        , (10,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            #Extract =and display the top class of the prediction\n",
    "            cv2.putText(bgr_frame, pose_class_status.split(' ')[0]\n",
    "                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            #Display the class probability inside the rectangle box\n",
    "            cv2.putText(bgr_frame, 'Probability'\n",
    "                        , (250,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            #Extract and dispthe maximum probability\n",
    "            cv2.putText(bgr_frame, str(round(pose_class_status_prob[np.argmax(pose_class_status_prob)],2))\n",
    "                        , (250,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            #Display the class probability inside the rectangle box\n",
    "            cv2.putText(bgr_frame, 'Counter'\n",
    "                        , (450,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            #Extract and dispthe maximum probability\n",
    "            cv2.putText(bgr_frame, str(counter)\n",
    "                        , (450,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    " \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        #Display the frames    \n",
    "        cv2.imshow('Results Feed', bgr_frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "#Display the overall result of the exercise            \n",
    "for i in range(counter):\n",
    "    x_total = 0\n",
    "    y_total = 0\n",
    "    total = 0\n",
    "    for list in rep_list[i]:\n",
    "        if list == \"Correct\":\n",
    "            x_total += 1\n",
    "        else:\n",
    "            y_total += 1\n",
    "        total += 1\n",
    "    \n",
    "    avg = (x_total / total) * 100\n",
    "    \n",
    "    if (avg >= 70):\n",
    "        feedback = \"Correct\"\n",
    "    else:\n",
    "        feedback = \"Incorrect\"\n",
    "    \n",
    "    print(\"The Rep No \", i + 1, \" was \", feedback)\n",
    "    print(\"Percentage of the correct average of the Rep:\" ,avg)\n",
    "\n",
    "\n",
    "sample_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a199944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
